'''
Let's create a classification model using the Titanic dataset as an example.
We will use the following libraries: pandas for data manipulation,
seaborn and matplotlib for visualization, and scikit-learn (sklearn)
for building and evaluating the machine learning model.

Here's a step-by-step guide:

Load the dataset
Explore the data
Preprocess the data
Build and evaluate the model


Step 1: Load the Dataset
We'll use the Titanic dataset from Seaborn's built-in datasets for simplicity.

'''

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load the Titanic dataset
titanic = sns.load_dataset('titanic')

# Display the first few rows of the dataset
print(titanic.head())


'''Step 2: Explore the Data
We('ll explore the dataset to understand '
'its structure and the relationships between features.)'''


# Display summary statistics
print(titanic.describe(include='all'))

# Check for missing values
print(titanic.isnull().sum())

# Visualize the distribution of survival
sns.countplot(x='survived', data=titanic)
plt.title('Survival Distribution')
plt.show()

# Visualize the relationship between features and survival
sns.catplot(x='sex', hue='survived', kind='count', data=titanic)
sns.catplot(x='class', hue='survived', kind='count', data=titanic)
plt.show()


'''Step 3: Preprocess the Data
We('ll handle missing values, encode categorical variables, and '
split the data into training and testing sets.)'''




# Handle missing values
titanic['age'].fillna(titanic['age'].median(), inplace=True)
titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)

# Drop columns that won't be used in the model
titanic.drop(columns=['deck', 'embark_town', 'alive'], inplace=True)

# Convert categorical variables to numerical using one-hot encoding
titanic = pd.get_dummies(titanic, columns=['sex', 'embarked', 'class', 'who', 'adult_male', 'alone'], drop_first=True)

# Define features (X) and target (y)
X = titanic.drop(columns='survived')
y = titanic['survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

'''  Step 4: Build and Evaluate the Model
# We'll use a Logistic Regression model for this classification task.'''

# Create and train the Logistic Regression model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')

# Plot the confusion matrix
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
